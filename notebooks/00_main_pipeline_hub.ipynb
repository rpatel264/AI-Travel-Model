{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Main Pipeline Hub - PDF Processing & Experimentation\n",
        "\n",
        "This notebook serves as the central experimentation and orchestration hub for the Chicago historical documents processing pipeline.\n",
        "\n",
        "## Workflow Overview\n",
        "1. **Load and Inspect PDFs** - Verify text extraction works correctly\n",
        "2. **Chunk Text** - Split documents into manageable pieces (~500 words)\n",
        "3. **Summarization & Testing** - Test Ollama prompts and settings\n",
        "4. **Prototype Pipeline** - Test full workflow: PDF â†’ Chunk â†’ Summarize â†’ Save\n",
        "5. **Documentation** - Document approach, assumptions, and special handling\n",
        "\n",
        "## Notes\n",
        "- Some PDFs may have unusual formatting - always verify extraction\n",
        "- Test different prompts and chunk sizes before committing to scripts\n",
        "- Use this notebook to debug issues interactively\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: Import libraries and configure paths\n",
        "import sys\n",
        "import json\n",
        "from pathlib import Path\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Add Chicago directory to Python path\n",
        "project_root = Path().parent\n",
        "chicago_dir = project_root / \"Chicago\"\n",
        "sys.path.insert(0, str(chicago_dir))\n",
        "\n",
        "print(f\"âœ“ Project root: {project_root}\")\n",
        "print(f\"âœ“ Chicago directory: {chicago_dir}\")\n",
        "print(f\"âœ“ Data/Raw directory: {chicago_dir / 'Data' / 'Raw'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import pipeline functions\n",
        "from pdf_pipeline import (\n",
        "    extract_pdf_text,\n",
        "    chunk_text,\n",
        "    summarize_with_ollama,\n",
        "    process_pdf,\n",
        "    ask_question_ollama\n",
        ")\n",
        "\n",
        "print(\"âœ“ All pipeline functions imported\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load and Inspect PDFs\n",
        "\n",
        "First, let's see what PDFs are available and inspect their contents to verify text extraction works correctly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List available PDFs in Data/Raw/\n",
        "raw_dir = chicago_dir / \"Data\" / \"Raw\"\n",
        "pdf_files = list(raw_dir.glob(\"*.pdf\"))\n",
        "\n",
        "print(f\"Found {len(pdf_files)} PDF file(s):\\n\")\n",
        "for i, pdf in enumerate(pdf_files, 1):\n",
        "    size_mb = pdf.stat().st_size / (1024 * 1024)\n",
        "    print(f\"{i}. {pdf.name} ({size_mb:.2f} MB)\")\n",
        "\n",
        "# Store the first PDF for processing (or select one)\n",
        "if pdf_files:\n",
        "    selected_pdf = pdf_files[0]\n",
        "    print(f\"\\nâœ“ Selected PDF: {selected_pdf.name}\")\n",
        "else:\n",
        "    print(\"\\nâš  No PDFs found. Please add PDFs to Chicago/Data/Raw/\")\n",
        "    selected_pdf = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract and inspect text from selected PDF\n",
        "if selected_pdf:\n",
        "    print(f\"Extracting text from: {selected_pdf.name}\\n\")\n",
        "    raw_text = extract_pdf_text(selected_pdf)\n",
        "    \n",
        "    print(f\"âœ“ Extracted {len(raw_text)} characters\")\n",
        "    print(f\"âœ“ Extracted {len(raw_text.split())} words\")\n",
        "    print(f\"âœ“ Extracted {len(raw_text.split('\\\\n'))} lines\")\n",
        "    \n",
        "    # Show first 500 characters to verify extraction quality\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FIRST 500 CHARACTERS (to verify extraction):\")\n",
        "    print(\"=\"*60)\n",
        "    print(raw_text[:500])\n",
        "    print(\"...\")\n",
        "    \n",
        "    # Check for potential issues\n",
        "    if len(raw_text) < 100:\n",
        "        print(\"\\nâš  WARNING: Very little text extracted. PDF may have:\")\n",
        "        print(\"  - Scanned images (needs OCR)\")\n",
        "        print(\"  - Unusual formatting\")\n",
        "        print(\"  - Protected/encrypted content\")\n",
        "else:\n",
        "    raw_text = None\n",
        "    print(\"No PDF selected\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Chunk Text\n",
        "\n",
        "Split the document into manageable chunks (~500 words each) for easier processing and summarization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test chunking with different sizes\n",
        "if raw_text:\n",
        "    # Test with default 500 words\n",
        "    chunks_500 = chunk_text(raw_text, max_tokens=500)\n",
        "    print(f\"âœ“ Created {len(chunks_500)} chunks (500 words each)\")\n",
        "    print(f\"  Average chunk size: {sum(len(c.split()) for c in chunks_500) / len(chunks_500):.1f} words\")\n",
        "    \n",
        "    # Show first chunk as example\n",
        "    if chunks_500:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"FIRST CHUNK PREVIEW:\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"Words: {len(chunks_500[0].split())}\")\n",
        "        print(f\"Characters: {len(chunks_500[0])}\")\n",
        "        print(f\"\\nContent (first 300 chars):\\n{chunks_500[0][:300]}...\")\n",
        "    \n",
        "    # Store chunks for next steps\n",
        "    chunks = chunks_500\n",
        "else:\n",
        "    chunks = []\n",
        "    print(\"No text to chunk\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Summarization & Testing\n",
        "\n",
        "Test Ollama summarization on a sample chunk. This allows you to:\n",
        "- Test different prompts\n",
        "- Adjust settings\n",
        "- Verify output quality\n",
        "- Debug issues before processing all chunks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test summarization on first chunk (if available)\n",
        "if chunks:\n",
        "    print(\"Testing Ollama summarization on first chunk...\\n\")\n",
        "    test_chunk = chunks[0]\n",
        "    \n",
        "    print(\"=\"*60)\n",
        "    print(\"ORIGINAL CHUNK (first 200 words):\")\n",
        "    print(\"=\"*60)\n",
        "    words = test_chunk.split()[:200]\n",
        "    print(\" \".join(words) + \"...\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"CALLING OLLAMA FOR SUMMARIZATION...\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # This may take a moment\n",
        "    summary = summarize_with_ollama(test_chunk)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"GENERATED SUMMARY:\")\n",
        "    print(\"=\"*60)\n",
        "    print(summary)\n",
        "    \n",
        "    print(\"\\nâœ“ Summarization test complete\")\n",
        "else:\n",
        "    print(\"No chunks available for testing\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Prototype Full Pipeline\n",
        "\n",
        "Run the complete workflow: PDF â†’ Extract â†’ Chunk â†’ Summarize â†’ Save JSON\n",
        "\n",
        "**Note:** This processes all chunks and may take some time depending on:\n",
        "- Number of chunks\n",
        "- Ollama response time\n",
        "- PDF size\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run full pipeline on selected PDF\n",
        "if selected_pdf:\n",
        "    print(f\"Running full pipeline on: {selected_pdf.name}\\n\")\n",
        "    print(\"This will:\")\n",
        "    print(\"  1. Extract text from PDF\")\n",
        "    print(\"  2. Chunk into ~500 word pieces\")\n",
        "    print(\"  3. Summarize each chunk with Ollama\")\n",
        "    print(\"  4. Save to Data/processed/ as JSON\\n\")\n",
        "    \n",
        "    # Uncomment to run full pipeline:\n",
        "    # enhanced_chunks = process_pdf(selected_pdf, save_chunks=True)\n",
        "    # print(f\"\\nâœ“ Pipeline complete! Processed {len(enhanced_chunks)} chunks\")\n",
        "    \n",
        "    print(\"âš  Uncomment the code above to run the full pipeline\")\n",
        "else:\n",
        "    print(\"No PDF selected\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4b: Process Full PDF (After Testing)\n",
        "\n",
        "Once you've verified the test works, you can process the full PDF or multiple PDFs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# OPTION 2: Process full PDF (uncomment after testing)\n",
        "# from engineering_pipeline import main\n",
        "\n",
        "# Process single PDF fully (all chunks)\n",
        "# results = main(pdf_path=selected_pdf, append=True, max_chunks_override=None)\n",
        "\n",
        "# Or process multiple PDFs\n",
        "# pdf_files = list(chicago_dir / \"Data\" / \"Raw\" / \"*.pdf\")\n",
        "# results = main(pdf_path=pdf_files, append=True)\n",
        "\n",
        "# print(f\"\\nâœ“ Processed {len(results)} total chunks\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Load and Inspect Processed Chunks\n",
        "\n",
        "Load previously processed chunks from JSON files to inspect results or use for querying.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process PDFs incrementally using engineering_pipeline\n",
        "from engineering_pipeline import main, list_available_pdfs\n",
        "\n",
        "# List all available PDFs\n",
        "all_pdfs = list_available_pdfs()\n",
        "print(f\"Available PDFs: {len(all_pdfs)}\")\n",
        "for i, pdf in enumerate(all_pdfs, 1):\n",
        "    size_mb = pdf.stat().st_size / (1024 * 1024)\n",
        "    print(f\"  {i}. {pdf.name} ({size_mb:.2f} MB)\")\n",
        "\n",
        "# Process specific PDFs (uncomment and modify as needed)\n",
        "# Example: Process first PDF only\n",
        "# if all_pdfs:\n",
        "#     results = main(pdf_path=all_pdfs[0], append=True, max_chunks_override=5)  # Test mode\n",
        "#     # results = main(pdf_path=all_pdfs[0], append=True)  # Full processing\n",
        "\n",
        "# Example: Process multiple PDFs\n",
        "# selected = [all_pdfs[0], all_pdfs[1]]  # Select which ones\n",
        "# results = main(pdf_path=selected, append=True)\n",
        "\n",
        "print(\"\\nðŸ’¡ Tip: Use engineering_pipeline.py interactively for easier selection\")\n",
        "print(\"   Run: cd Chicago && python engineering_pipeline.py\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load processed chunks from Data/processed/\n",
        "processed_dir = chicago_dir / \"Data\" / \"processed\"\n",
        "json_files = list(processed_dir.glob(\"*_chunks.json\"))\n",
        "\n",
        "print(f\"Found {len(json_files)} processed chunk file(s):\\n\")\n",
        "for i, json_file in enumerate(json_files, 1):\n",
        "    with open(json_file, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "    print(f\"{i}. {json_file.name}\")\n",
        "    print(f\"   - {len(data)} chunks\")\n",
        "    print(f\"   - Source: {data[0]['pdf_path'] if data else 'N/A'}\\n\")\n",
        "\n",
        "# Load the first file as example\n",
        "if json_files:\n",
        "    with open(json_files[0], 'r', encoding='utf-8') as f:\n",
        "        loaded_chunks = json.load(f)\n",
        "    \n",
        "    print(f\"âœ“ Loaded {len(loaded_chunks)} chunks from {json_files[0].name}\")\n",
        "    \n",
        "    # Show sample chunk\n",
        "    if loaded_chunks:\n",
        "        sample = loaded_chunks[0]\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"SAMPLE PROCESSED CHUNK:\")\n",
        "        print(\"=\"*60)\n",
        "        print(f\"ID: {sample['id']}\")\n",
        "        print(f\"Position: {sample['chunk_position']}\")\n",
        "        print(f\"\\nSummary:\\n{sample['summary']}\")\n",
        "        print(f\"\\nText preview (first 200 chars):\\n{sample['text'][:200]}...\")\n",
        "else:\n",
        "    loaded_chunks = []\n",
        "    print(\"No processed chunks found. Run the pipeline first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Test Query/Retrieval\n",
        "\n",
        "Test the retrieval system by asking questions about the processed chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test query functionality\n",
        "if 'loaded_chunks' in locals() and loaded_chunks:\n",
        "    # Test queries\n",
        "    test_queries = [\n",
        "        \"mayor chicago\",\n",
        "        \"architecture\",\n",
        "        \"history\",\n",
        "        \"fire\"\n",
        "    ]\n",
        "    \n",
        "    print(\"Testing retrieval with sample queries:\\n\")\n",
        "    for query in test_queries:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"QUERY: '{query}'\")\n",
        "        print('='*60)\n",
        "        result = ask_question_ollama(query, loaded_chunks)\n",
        "        if result:\n",
        "            print(f\"\\nâœ“ Found match\")\n",
        "        else:\n",
        "            print(f\"\\nâœ— No match found\")\n",
        "else:\n",
        "    print(\"No chunks loaded. Process a PDF first or load existing chunks.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Documentation & Notes\n",
        "\n",
        "Use this section to document:\n",
        "- Special handling for specific PDFs\n",
        "- Issues encountered and solutions\n",
        "- Optimal settings discovered\n",
        "- Assumptions and approach\n",
        "\n",
        "### Known Issues & Solutions\n",
        "\n",
        "**Issue:** Some PDFs have unusual formatting\n",
        "- **Solution:** Verify extraction in Step 1, adjust extraction method if needed\n",
        "\n",
        "**Issue:** Ollama may be slow for large documents\n",
        "- **Solution:** Process in batches, save progress frequently\n",
        "\n",
        "**Issue:** Chunks may split sentences awkwardly\n",
        "- **Solution:** Consider sentence-aware chunking for better summaries\n",
        "\n",
        "### Optimal Settings\n",
        "\n",
        "- **Chunk size:** 500 words works well for most documents\n",
        "- **Ollama model:** llama3.1:8b provides good balance of speed and quality\n",
        "- **Summary format:** 2-4 bullet points keeps summaries concise\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "Once confident with the workflow:\n",
        "1. Refactor tested code into `engineering_pipeline.py`\n",
        "2. Update `pdf_pipeline.py` with any improvements\n",
        "3. Enhance `query_chunks.py` and `retrieval_v2.py` based on testing\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
